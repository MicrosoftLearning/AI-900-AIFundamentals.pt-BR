{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Detecção de Objetos\r\n",
        "\r\n",
        "*Detecção de objetos* é uma forma de pesquisa visual computacional em que um modelo de machine learning é treinado para classificar instâncias individuais de objetos em uma imagem e indicar uma *caixa delimitadora* que marca a localização delas. Pense nisso como um avanço da *classificação de imagem* (em que o modelo responde à pergunta \"o que tem nessa imagem?\") para criar soluções em que podemos questionar ao modelo \"quais objetos estão nessa imagem e onde eles se encontram?\".\r\n",
        "\r\n",
        "![Um robô identificando uma fruta](./images/object-detection.jpg)\r\n",
        "\r\n",
        "Por exemplo, uma mercearia pode usar um modelo de detecção de objetos para implementar um sistema automatizado de finalização de compra que examina uma esteira rolante usando uma câmera e identifica itens específicos, sem a necessidade de colocar cada um deles sobre a esteira e escaneá-los individualmente.\r\n",
        "\r\n",
        "O serviço cognitivo de **Visão Personalizada** do Microsoft Azure oferece uma solução baseada em nuvem para criar e publicar modelos personalizados de detecção de objetos.\r\n",
        "\r\n",
        "## Criar um recurso de Visão Personalizada\r\n",
        "\r\n",
        "Para usar o serviço de Visão Personalizada, é preciso ter um recurso do Azure que você possa usar para treinar um modelo, e um recurso com o qual possa publicá-lo para os aplicativos utilizarem. Você pode usar o mesmo recurso para as duas tarefas, ou recorrer a recursos diferentes para alocar os custos separadamente, desde que ambos sejam criados na mesma região. Esses recursos podem ser gerais dos **Serviços Cognitivos** ou específicos de **Visão Personalizada**. Use as instruções a seguir para criar um novo recurso de **Visão personalizada** (ou use um recurso existente, caso já tenha um).\r\n",
        "\r\n",
        "1. Em uma nova guia do navegador, abra o portal do Azure em [https://portal.azure.com](https://portal.azure.com) e conecte-se usando a conta Microsoft associada à sua assinatura do Azure.\r\n",
        "2. Selecione o botão **&#65291;Criar um recurso**, procure a opção *Visão personalizada* e crie um recurso de **Visão personalizada** com as seguintes configurações:\r\n",
        "    - **Opções de criação**: Ambos\r\n",
        "    - **Assinatura**: *sua assinatura do Azure*\r\n",
        "    - **Grupo de recursos**: *Selecione ou crie um grupo de recursos com um nome exclusivo*\r\n",
        "    - **Nome**: *Insira um nome exclusivo*\r\n",
        "    - **Local de treinamento**: *Escolha qualquer região disponível*\r\n",
        "    - **Tipo de preço de treinamento**: F0\r\n",
        "    - **Local de previsão**: *o mesmo que o local de treinamento*\r\n",
        "    - **Tipo de preço de previsão **: F0\r\n",
        "\r\n",
        "    >**Observação**: se você já tiver um serviço de visão personalizada F0 na sua assinatura, selecione **S0** nesse.\r\n",
        "\r\n",
        "3. Aguarde até o recurso ser criado.\r\n",
        "\r\n",
        "## Criar um projeto de Visão Personalizada\r\n",
        "\r\n",
        "Para treinar um modelo de detecção de objetos, é necessário criar um projeto de Visão Personalizada com base no seu recurso de treinamento. Para isso, você usará o portal Visão Personalizada.\r\n",
        "\r\n",
        "1. Em uma nova guia do navegador, abra o portal de Visão Personalizada em [https://customvision.ai](https://customvision.ai) e conecte-se usando a conta Microsoft associada à sua assinatura do Azure.\r\n",
        "2. Crie um novo projeto com as seguintes configurações:\r\n",
        "    - **Nome**: detecção de mercadorias\r\n",
        "    - **Descrição**: detecção de objetos para mercearias.\r\n",
        "    - **Recurso**: *o recurso de Visão Personalizada que você criou anteriormente*\r\n",
        "    - **Tipos de projeto**: Detecção de Objetos\r\n",
        "    - **Domínios**: Geral\r\n",
        "3. Aguarde até o projeto ser criado e aberto no navegador.\r\n",
        "\r\n",
        "## Adicionar e marcar imagens\r\n",
        "\r\n",
        "Para treinar um modelo de detecção de objetos, é preciso fazer upload de imagens que contenham as classes que você quer que o modelo identifique e marcá-las para indicar as caixas delimitadoras de cada instância do objeto.\r\n",
        "\r\n",
        "1. Faça download e extraia as imagens de treinamento em https://aka.ms/fruit-objects. A pasta extraída contém uma coleção de imagens de frutas. **Observação:** como uma solução alternativa temporária, caso você não consiga acessar as imagens de treinamento, acesse https://www.github.com e depois https://aka.ms/fruit-objects. \r\n",
        "2. No portal Visão personalizada [https://customvision.ai](https://customvision.ai), garanta que esteja trabalhando em sua detecção de objeto project _Grocery Detection_. Em seguida, selecione **Adicionar imagens** e faça upload de todas as imagens da pasta extraída.\r\n",
        "\r\n",
        "![Faça o upload das imagens baixadas clicando em Adicionar imagens.](./images/fruit-upload.jpg)\r\n",
        "\r\n",
        "3. Depois que as imagens tiverem sido carregadas, selecione a primeira delas para abrir.\r\n",
        "4. Mantenha o mouse sobre qualquer objeto da imagem até que uma região detectada automaticamente seja exibida, como na imagem abaixo. Em seguida, selecione o objeto e, se necessário, redimensione a região para envolvê-lo.\r\n",
        "\r\n",
        "![A região padrão de um objeto](./images/object-region.jpg)\r\n",
        "\r\n",
        "Outra opção é simplesmente arrastar o cursor ao redor de um objeto para criar uma região.\r\n",
        "\r\n",
        "5. Quando a região envolver o objeto, adicione uma nova marca com o tipo de objeto apropriado (*maçã*, *banana* ou *laranja*), como mostrado aqui:\r\n",
        "\r\n",
        "![Um objeto marcado em uma imagem](./images/object-tag.jpg)\r\n",
        "\r\n",
        "6. Selecione e marque os outros objetos da imagem, redimensionando as regiões e adicionando novas marcas conforme o necessário.\r\n",
        "\r\n",
        "![Dois objetos marcados em uma imagem](./images/object-tags.jpg)\r\n",
        "\r\n",
        "7. Use o link **>** à direita para ir para a próxima imagem e marcar os objetos encontrados nela. Continue trabalhando em toda a coleção de imagens, marcando todas as maçãs, bananas e laranjas.\r\n",
        "\r\n",
        "8. Quando terminar de marcar a última imagem, feche o editor de **Detalhes da imagem** e, na página **Imagens de treinamento**, na seção **Marcas**, selecione **Marcadas** para ver todas as suas imagens com marcações:\r\n",
        "\r\n",
        "![Imagens marcadas em um projeto](./images/tagged-images.jpg)\r\n",
        "\r\n",
        "## Treinar e testar um modelo\r\n",
        "\r\n",
        "Agora que já marcou as imagens do projeto, você está pronto para treinar um modelo.\r\n",
        "\r\n",
        "1. No projeto de Visão Personalizada, clique em **Treinar** para treinar um modelo de detecção de objetos usando as imagens com marcas. Selecione a opção **Treinamento rápido**.\r\n",
        "2. Aguarde a conclusão do treinamento (o processo pode levar dez minutos ou mais) e, em seguida, analise as métricas de desempenho *Precisão*, *Recall* e *mAP* – elas medem a precisão da previsão do modelo de classificação e devem ser altas.\r\n",
        "3. No canto superior direito da página, clique em **Teste rápido**. Em seguida, na caixa **URL da imagem**, insira `https://aka.ms/apple-orange` e veja a precisão que será gerada. Depois, feche a janela de **Teste rápido**.\r\n",
        "\r\n",
        "## Publicar e consumir o modelo de detecção de objetos\r\n",
        "\r\n",
        "Agora você está pronto para publicar seu modelo treinado e usá-lo em um aplicativo cliente.\r\n",
        "\r\n",
        "1. No canto superior esquerdo da página **Desempenho**, clique em **&#128504; Publicar** para publicar o modelo treinado com as seguintes configurações:\r\n",
        "    - **Nome do modelo**: detecção de frutas\r\n",
        "    - **Recurso de previsão**: *seu recurso de **previsão de** Visão Personalizada*.\r\n",
        "\r\n",
        "### (!) Verificação \r\n",
        "Você usou o mesmo nome do modelo: **detecção de frutas**? \r\n",
        "\r\n",
        "2. Depois de publicar, clique no ícone de *configurações* (&#9881;) no canto superior direito da página de **Desempenho** para visualizar as configurações do projeto. Na seção **Geral** (à esquerda), copie a **ID do projeto**. Role para baixo e cole-o na célula de código debaixo da etapa 5, substituindo **YOUR_PROJECT_ID**. \r\n",
        "\r\n",
        "> (*Se você tiver usado um recurso dos **Serviços Cognitivos** em vez de criar um recurso de **Visão Personalizada** no início deste exercício, poderá copiar a chave e o ponto de extremidade correspondentes no lado direito das configurações do projeto, colar esses dados na célula de código abaixo e executá-la para ver os resultados. Caso contrário, continue concluindo as etapas abaixo para obter a chave e o ponto de extremidade do seu recurso de previsão de Visão Personalizada*).\r\n",
        "\r\n",
        "3. No canto superior esquerdo da página **Configurações do projeto**, clique no ícone das *Galeria de projetos* (&#128065;) para voltar à página inicial do portal de Visão Personalizada, onde seu projeto estará listado agora.\r\n",
        "\r\n",
        "4. Na página inicial do portal de Visão Personalizada, no canto superior direito, clique no ícone de *configurações* (&#9881;) para visualizar as configurações do seu serviço de Visão Personalizada. Depois, na seção **Recursos**, expanda o recurso de *previsão* (<u>não</u> o de treinamento) e copie os valores correspondentes de **Chave** e **Ponto de extremidade** na célula de código abaixo da etapa 5, substituindo **YOUR_KEY** e **YOUR_ENDPOINT**.\r\n",
        "\r\n",
        "### (!) Verificação \r\n",
        "Caso esteja usando um recurso de **Visão Personalizada**, você usou o recurso de **previsão** (<u>não</u> o de treinamento)?\r\n",
        "\r\n",
        "5. Execute a célula de código abaixo clicando no botão Executar célula <span>&#9655;</span> (no canto superior esquerdo) para definir as variáveis como os valores de ID do projeto, chave e ponto de extremidade."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "project_id = 'YOUR_PROJECT_ID' # Replace with your project ID\r\n",
        "cv_key = 'YOUR_KEY' # Replace with your prediction resource primary key\r\n",
        "cv_endpoint = 'YOUR_ENDPOINT' # Replace with your prediction resource endpoint\r\n",
        "\r\n",
        "model_name = 'detect-produce' # this must match the model name you set when publishing your model iteration exactly (including case)!\r\n",
        "print('Ready to predict using model {} in project {}'.format(model_name, project_id))"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1599692485387
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora você pode usar sua chave e ponto de extremidade com um cliente de Visão Personalizada para se conectar ao modelo de detecção de objetos de visão personalizada.\r\n",
        "\r\n",
        "Execute a célula de código abaixo, que usa o seu modelo para detectar itens de hortifruti individuais em uma imagem.\r\n",
        "\r\n",
        "> **Observação**: Não se preocupe muito com os detalhes do código. Ele usa o SDK do Python para o serviço de Visão Personalizada para enviar uma imagem para o seu modelo e recuperar previsões de objetos detectados. Cada previsão consiste em um nome de classe (*maçã*, *banana* ou *laranja*) e coordenadas de uma *caixa delimitadora* que indicam onde o objeto previsto foi detectado na imagem. O código usa essas informações para traçar uma caixa rotulada ao redor de cada objeto da imagem."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\r\n",
        "from msrest.authentication import ApiKeyCredentials\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "from PIL import Image, ImageDraw, ImageFont\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "# Load a test image and get its dimensions\r\n",
        "test_img_file = os.path.join('data', 'object-detection', 'produce.jpg')\r\n",
        "test_img = Image.open(test_img_file)\r\n",
        "test_img_h, test_img_w, test_img_ch = np.array(test_img).shape\r\n",
        "\r\n",
        "# Get a prediction client for the object detection model\r\n",
        "credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": cv_key})\r\n",
        "predictor = CustomVisionPredictionClient(endpoint=cv_endpoint, credentials=credentials)\r\n",
        "\r\n",
        "print('Detecting objects in {} using model {} in project {}...'.format(test_img_file, model_name, project_id))\r\n",
        "\r\n",
        "# Detect objects in the test image\r\n",
        "with open(test_img_file, mode=\"rb\") as test_data:\r\n",
        "    results = predictor.detect_image(project_id, model_name, test_data)\r\n",
        "\r\n",
        "# Create a figure to display the results\r\n",
        "fig = plt.figure(figsize=(8, 8))\r\n",
        "plt.axis('off')\r\n",
        "\r\n",
        "# Display the image with boxes around each detected object\r\n",
        "draw = ImageDraw.Draw(test_img)\r\n",
        "lineWidth = int(np.array(test_img).shape[1]/100)\r\n",
        "object_colors = {\r\n",
        "    \"apple\": \"lightgreen\",\r\n",
        "    \"banana\": \"yellow\",\r\n",
        "    \"orange\": \"orange\"\r\n",
        "}\r\n",
        "for prediction in results.predictions:\r\n",
        "    color = 'white' # default for 'other' object tags\r\n",
        "    if (prediction.probability*100) > 50:\r\n",
        "        if prediction.tag_name in object_colors:\r\n",
        "            color = object_colors[prediction.tag_name]\r\n",
        "        left = prediction.bounding_box.left * test_img_w \r\n",
        "        top = prediction.bounding_box.top * test_img_h \r\n",
        "        height = prediction.bounding_box.height * test_img_h\r\n",
        "        width =  prediction.bounding_box.width * test_img_w\r\n",
        "        points = ((left,top), (left+width,top), (left+width,top+height), (left,top+height),(left,top))\r\n",
        "        draw.line(points, fill=color, width=lineWidth)\r\n",
        "        plt.annotate(prediction.tag_name + \": {0:.2f}%\".format(prediction.probability * 100),(left,top), backgroundcolor=color)\r\n",
        "plt.imshow(test_img)\r\n"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1599692585672
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veja as previsões resultantes, que mostram os objetos detectados e a probabilidade de cada previsão."
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}