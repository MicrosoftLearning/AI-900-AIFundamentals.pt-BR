{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reconhecimento óptico de caracteres\r\n",
        "\r\n",
        "![Um robô lendo um jornal](./images/ocr.jpg)\r\n",
        "\r\n",
        "Um desafio comum da pesquisa visual computacional é detectar e interpretar textos em uma imagem. Esse tipo de processamento é chamado normalmente de *reconhecimento óptico de caracteres* (OCR).\r\n",
        "\r\n",
        "## Use o serviço de Pesquisa Visual Computacional para ler textos de uma imagem\r\n",
        "\r\n",
        "O serviço cognitivo de **Pesquisa Visual Computacional** oferece suporte a tarefas de OCR, entre elas:\r\n",
        "\r\n",
        "- Uma API de **OCR** que pode ser usada para ler textos em vários idiomas. Essa API pode ser usada de maneira síncrona e funciona bem quando você precisa detectar e ler uma pequena quantidade de texto em uma imagem.\r\n",
        "- Uma API de **Leitura** otimizada para documentos maiores. Essa API é usada de maneira assíncrona e compatível com texto impresso ou manuscrito.\r\n",
        "\r\n",
        "Você pode usar esse serviço criando um recurso de **Pesquisa Visual Computacional** ou de **Serviços Cognitivos**.\r\n",
        "\r\n",
        "Caso ainda não tenha feito isso, crie um recurso dos **Serviços Cognitivos** na sua assinatura do Azure.\r\n",
        "\r\n",
        "> **Observação**: Se você já tiver um recurso dos Serviços Cognitivos, abra a página de **Início Rápido** no portal do Azure e copie a respectiva chave e o ponto de extremidade para a célula abaixo. Caso contrário, siga as etapas abaixo para criar um.\r\n",
        "\r\n",
        "1. Em outra guia do navegador, abra o portal do Azure em https://portal.azure.com e conecte-se com a sua conta Microsoft.\r\n",
        "\r\n",
        "2. Clique no botão **&#65291;Criar um recurso**, procure *Serviços Cognitivos* e crie um recurso dos **Serviços Cognitivos** com as configurações abaixo:\r\n",
        "    - **Assinatura**: *sua assinatura do Azure*.\r\n",
        "    - **Grupo de recursos**: *Selecione ou crie um grupo de recursos com um nome exclusivo*.\r\n",
        "    - **Região**: *Escolha qualquer região disponível*:\r\n",
        "    - **Nome**: *Insira um nome exclusivo*.\r\n",
        "    - **Tipo de preço**: S0\r\n",
        "    - **Confirmo que li e entendi os avisos**: Selecionado.\r\n",
        "3. Aguarde até que a implantação seja concluída. Depois, entre em seu recurso dos Serviços Cognitivos e, na página **Visão geral**, clique no link para gerenciar as chaves do serviço. Você precisará do ponto de extremidade e das chaves para se conectar aos seus recursos dos Serviços Cognitivos em aplicativos clientes.\r\n",
        "\r\n",
        "### Obter a chave e o ponto de extremidade do seu recurso dos Serviços Cognitivos\r\n",
        "\r\n",
        "Para usar seu recurso dos Serviços Cognitivos, os aplicativos clientes precisam do ponto de extremidade e da chave de autenticação:\r\n",
        "\r\n",
        "1. No portal do Azure, na página **Chaves e ponto de extremidade** do seu recurso dos Serviços Cognitivos, copie a **Chave 1** do recurso e cole no código abaixo, substituindo **YOUR_COG_KEY**.\r\n",
        "2. Copie o **ponto de extremidade** do recurso e cole no código abaixo, substituindo **YOUR_COG_ENDPOINT**.\r\n",
        "3. Execute o código na célula abaixo clicando no botão **Executar célula** (&#9655;) (à esquerda)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cog_key = 'YOUR_COG_KEY'\n",
        "cog_endpoint = 'YOUR_COG_ENDPOINT'\n",
        "\n",
        "print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599694246277
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora que você já configurou a chave e o ponto de extremidade, poderá usar o recurso de serviço de Pesquisa Visual Computacional para extrair texto de uma imagem.\r\n",
        "\r\n",
        "Vamos começar com a API de **OCR**, que possibilita a análise síncrona e a leitura do texto contido em uma imagem. Neste caso, você tem uma imagem publicitária da empresa varejista fictícia Northwind Traders que inclui texto. Execute a célula abaixo para ler. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Get a client for the computer vision service\r\n",
        "computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# Read the image file\r\n",
        "image_path = os.path.join('data', 'ocr', 'advert.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Use the Computer Vision service to find text in the image\r\n",
        "read_results = computervision_client.recognize_printed_text_in_stream(image_stream)\n",
        "\n",
        "# Process the text line by line\r\n",
        "for region in read_results.regions:\n",
        "    for line in region.lines:\n",
        "\n",
        "        # Read the words in the line of text\n",
        "        line_text = ''\n",
        "        for word in line.words:\n",
        "            line_text += word.text + ' '\n",
        "        print(line_text.rstrip())\n",
        "\n",
        "# Open image to display it.\r\n",
        "fig = plt.figure(figsize=(7, 7))\n",
        "img = Image.open(image_path)\n",
        "draw = ImageDraw.Draw(img)\n",
        "plt.axis('off')\n",
        "plt.imshow(img)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599694257280
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O texto encontrado na imagem é organizado em uma estrutura hierárquica de regiões, linhas e palavras. O código lê essas informações para recuperar os resultados.\r\n",
        "\r\n",
        "Nos resultados, veja o texto que foi lido acima da imagem. \r\n",
        "\r\n",
        "## Exibir caixas delimitadoras\r\n",
        "\r\n",
        "O resultado também inclui coordenadas de *caixa delimitadora* para as linhas do texto e palavras individuais encontradas na imagem. Execute a célula abaixo para ver as caixas delimitadoras das linhas de texto na imagem publicitária que você recuperou acima."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Open image to display it.\r\n",
        "fig = plt.figure(figsize=(7, 7))\n",
        "img = Image.open(image_path)\n",
        "draw = ImageDraw.Draw(img)\n",
        "\n",
        "# Process the text line by line\r\n",
        "for region in read_results.regions:\n",
        "    for line in region.lines:\n",
        "\n",
        "        # Show the position of the line of text\n",
        "        l,t,w,h = list(map(int, line.bounding_box.split(',')))\n",
        "        draw.rectangle(((l,t), (l+w, t+h)), outline='magenta', width=5)\n",
        "\n",
        "        # Read the words in the line of text\n",
        "        line_text = ''\n",
        "        for word in line.words:\n",
        "            line_text += word.text + ' '\n",
        "        print(line_text.rstrip())\n",
        "\n",
        "# Show the image with the text locations highlighted\r\n",
        "plt.axis('off')\n",
        "plt.imshow(img)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599694266106
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No resultado, a caixa delimitadora de cada linha do texto é exibida como um retângulo na imagem.\r\n",
        "\r\n",
        "## Usar a API de Leitura\r\n",
        "\r\n",
        "A API de OCR usada anteriormente funciona bem para imagens com pequenas quantidades de texto. Quando precisar ler corpos de texto maiores, como documentos digitalizados, você poderá usar a API de **Leitura**. Para isso, siga esse processo de várias etapas:\r\n",
        "\r\n",
        "1. Envie uma imagem para o serviço de Pesquisa Visual Computacional para ser lida e analisada de maneira assíncrona.\r\n",
        "2. Aguarde até que a operação de análise seja concluída.\r\n",
        "3. Recupere os resultados da análise.\r\n",
        "\r\n",
        "Execute a célula abaixo para usar esse processo para ler o texto de uma carta escaneada para o gerente de uma loja da Northwind Traders."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import time\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Read the image file\r\n",
        "image_path = os.path.join('data', 'ocr', 'letter.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Get a client for the computer vision service\r\n",
        "computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# Submit a request to read printed text in the image and get the operation ID\r\n",
        "read_operation = computervision_client.read_in_stream(image_stream,\n",
        "                                                      raw=True)\n",
        "operation_location = read_operation.headers[\"Operation-Location\"]\n",
        "operation_id = operation_location.split(\"/\")[-1]\n",
        "\n",
        "# Wait for the asynchronous operation to complete\r\n",
        "while True:\n",
        "    read_results = computervision_client.get_read_result(operation_id)\n",
        "    if read_results.status not in [OperationStatusCodes.running]:\n",
        "        break\n",
        "    time.sleep(1)\n",
        "\n",
        "# If the operation was successfuly, process the text line by line\r\n",
        "if read_results.status == OperationStatusCodes.succeeded:\n",
        "    for result in read_results.analyze_result.read_results:\n",
        "        for line in result.lines:\n",
        "            print(line.text)\n",
        "\n",
        "# Open image and display it.\r\n",
        "print('\\n')\n",
        "fig = plt.figure(figsize=(12,12))\n",
        "img = Image.open(image_path)\n",
        "plt.axis('off')\n",
        "plt.imshow(img)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599694312346
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examine os resultados. Há uma transcrição completa da carta, que consiste principalmente em texto impresso com uma assinatura manuscrita. A imagem original da carta é exibida abaixo dos resultados do OCR (talvez você precise rolar a tela para vê-la).\r\n",
        "\r\n",
        "## Ler texto manuscrito\r\n",
        "\r\n",
        "No exemplo anterior, a solicitação para analisar a imagem especificou um modo de reconhecimento de texto que otimizou a operação para texto *impresso*. Observe que, apesar disso, a assinatura manuscrita foi lida.\r\n",
        "\r\n",
        "Essa capacidade de ler texto manuscrito é extremamente útil. Por exemplo, suponha que você tenha escrito um bilhete que contém uma lista de compras e queira usar um aplicativo no celular para ler as anotações e transcrever o texto do papel.\r\n",
        "\r\n",
        "Execute a célula abaixo para ver um exemplo de operação de leitura de uma lista de compras manuscrita."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import time\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Read the image file\r\n",
        "image_path = os.path.join('data', 'ocr', 'note.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Get a client for the computer vision service\r\n",
        "computervision_client = ComputerVisionClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# Submit a request to read printed text in the image and get the operation ID\r\n",
        "read_operation = computervision_client.read_in_stream(image_stream,\n",
        "                                                      raw=True)\n",
        "operation_location = read_operation.headers[\"Operation-Location\"]\n",
        "operation_id = operation_location.split(\"/\")[-1]\n",
        "\n",
        "# Wait for the asynchronous operation to complete\r\n",
        "while True:\n",
        "    read_results = computervision_client.get_read_result(operation_id)\n",
        "    if read_results.status not in [OperationStatusCodes.running]:\n",
        "        break\n",
        "    time.sleep(1)\n",
        "\n",
        "# If the operation was successfuly, process the text line by line\r\n",
        "if read_results.status == OperationStatusCodes.succeeded:\n",
        "    for result in read_results.analyze_result.read_results:\n",
        "        for line in result.lines:\n",
        "            print(line.text)\n",
        "\n",
        "# Open image and display it.\r\n",
        "print('\\n')\n",
        "fig = plt.figure(figsize=(12,12))\n",
        "img = Image.open(image_path)\n",
        "plt.axis('off')\n",
        "plt.imshow(img)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599694340593
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mais informações\r\n",
        "\r\n",
        "Para mais informações sobre como usar o serviço de Pesquisa Visual Computacional para OCR, consulte a [documentação da Pesquisa Visual Computacional](https://docs.microsoft.com/pt-br/azure/cognitive-services/computer-vision/concept-recognizing-text)"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}