{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reconhecimento vocal\r\n",
        "\r\n",
        "Cada vez mais, esperamos que os computadores sejam capazes de usar IA para reconhecer comandos falados ou digitados em linguagem natural. Por exemplo, você pode querer implementar um sistema de automação residencial que permita controlar os dispositivos da sua casa usando comandos de voz como \"acenda a luz\" ou \"ligue o ventilador\" usando um dispositivo alimentado por IA que reconheça o comando e realize as ações apropriadas.\r\n",
        "\r\n",
        "![Um robô ouvindo](./images/language_understanding.jpg)\r\n",
        "\r\n",
        "## Criar recursos de criação e previsão\r\n",
        "\r\n",
        "Os Serviços Cognitivos da Microsoft incluem o serviço de Reconhecimento vocal, que permite que você defina *intenções* que são aplicadas a *entidades* de acordo com *declarações*. \r\n",
        "\r\n",
        "Para usar o serviço de Reconhecimento vocal, é preciso ter dois tipos de recursos:\r\n",
        "\r\n",
        "- Um recurso de *criação*: usado para definir, treinar e testar o modelo de linguagem. Ele precisa ser um recurso de **Reconhecimento vocal - Criação** na sua assinatura do Azure.\r\n",
        "- Um recurso de *previsão*: usado para publicar as solicitações de modelo e identificador dos aplicativos clientes que o utilizam. Pode ser um recurso de **Reconhecimento vocal** ou **Serviços Cognitivos** na sua assinatura do Azure.\r\n",
        "\r\n",
        "É possível usar um recurso de **Reconhecimento vocal** ou  **Serviços Cognitivos** para *publicar* um aplicativo de reconhecimento de linguagem, mas você precisará gerar um recurso de **Reconhecimento vocal** separado para *criar* o aplicativo.\r\n",
        "\r\n",
        "> **Importante**: recursos de criação precisam ser criados em uma das três *regiões* (Europa, Austrália ou EUA). Modelos gerados em recursos de criação europeus ou australianos só podem ser implantados em recursos de previsão da Europa ou da Austrália, respectivamente. Modelos gerados em recursos de criação norte-americanos podem ser implantados em recursos de previsão de qualquer localização do Azure, exceto Europa e Austrália. Consulte a [documentação sobre regiões de criação e publicação](https://docs.microsoft.com/azure/cognitive-services/luis/luis-reference-regions) para mais detalhes sobre como combinar locais de criação e previsão.\r\n",
        "\r\n",
        "1. Em outra guia do navegador, abra o portal do Azure em [https://portal.azure.com](https://portal.azure.com), entrando com sua conta Microsoft.\r\n",
        "2. Clique em **+ Criar um recurso** e pesquise por * Reconhecimento vocal*.\r\n",
        "3. Na lista de serviços, clique em **Reconhecimento vocal**.\r\n",
        "4. Na lâmina do **Reconhecimento vocal**, clique em **Criar**.\r\n",
        "5. Na lâmina **Criar**, insira os detalhes abaixo e clique em **Criar**\r\n",
        "   - **Opção de criação**: Ambos\r\n",
        "   - **Nome**: *um nome exclusivo para seu serviço*\r\n",
        "   - **Assinatura**: *Selecione sua assinatura do Azure*\r\n",
        "   - **Grupo de recursos**: *Selecione um grupo de recursos existente ou crie um novo*\r\n",
        "   - **Local de criação**: *Selecione seu local preferido*\r\n",
        "   - **Tipo de preço de criação**: F0\r\n",
        "   - **Local de previsão**: *Escolha um local na mesma região do local de criação*\r\n",
        "   - **Tipo de preço de previsão **: F0\r\n",
        "   \r\n",
        "6. Aguarde até que os recursos sejam criados e observe que dois recursos de Reconhecimento vocal foram provisionados: um para criação e outro para previsão. Você pode visualizá-los navegando até o grupo de recursos onde os criou.\r\n",
        "\r\n",
        "### Criar um Aplicativo de Reconhecimento vocal\r\n",
        "\r\n",
        "Para implementar o reconhecimento de linguagem natural com Reconhecimento vocal, primeiro é preciso criar o aplicativo e, em seguida, adicionar entidades, intenções e declarações para definir os comandos que você quer que ele reconheça:\r\n",
        "\r\n",
        "1. Em uma nova guia do navegador, abra o portal de Reconhecimento vocal da região de criação em que você produziu seu recurso de criação:\r\n",
        "    - EUA: [https://www.luis.ai](https://www.luis.ai)\r\n",
        "    - Europa: [https://eu.luis.ai](https://eu.luis.ai)\r\n",
        "    - Austrália: [https://au.luis.ai](https://au.luis.ai)\r\n",
        "\r\n",
        "2. Entre usando a conta Microsoft associada à sua assinatura do Azure. Se essa for a primeira vez que você está se conectando ao portal de Reconhecimento vocal, talvez seja necessário conceder algumas permissões para o aplicativo acessar os detalhes da sua conta. Depois, conclua as etapas de *Boas-vindas* selecionando o recurso de criação de Reconhecimento vocal existente que você acabou de criar na assinatura do Azure. \r\n",
        "\r\n",
        "3. Abra a página **Aplicativos de conversa** e selecione sua assinatura e o recurso de criação de Reconhecimento vocal. Em seguida, crie um novo aplicativo para conversa com a seguinte configuração:\r\n",
        "   - **Nome**: Home Automation\r\n",
        "   - **Cultura**: português (*se essa opção não estiver disponível, deixe em branco*)\r\n",
        "   - **Descrição**: automação residencial simples\r\n",
        "   - **Recurso de previsão**: *seu recurso de previsão de Reconhecimento vocal*\r\n",
        "\r\n",
        "4. Se o painel com as dicas para criar um aplicativo eficaz de Reconhecimento vocal for exibido, feche-o.\r\n",
        "\r\n",
        "### Criar uma entidade\r\n",
        "\r\n",
        "Uma *entidade* é algo que o seu modelo de linguagem consegue identificar e usar para fazer alguma coisa. Neste caso, seu aplicativo de Reconhecimento vocal será usado para controlar vários *dispositivos* no escritório, como lâmpadas ou ventiladores. Por isso, você criará uma entidade *dispositivo*, que inclui uma lista dos tipos de dispositivos com os quais você quer que o aplicativo funcione. Para cada tipo de dispositivo, você criará uma sublista identificando o nome do dispositivo (por exemplo, *luz*) e os sinônimos que podem ser usados para se referir a esse tipo de dispositivo (por exemplo, *lâmpada*).\r\n",
        "\r\n",
        "1. Na página de Reconhecimento vocal do seu aplicativo, no painel à esquerda, clique em **Entidades**. Depois, clique em **Criar** e crie uma nova entidade chamada **dispositivo**. Selecione o tipo **Lista** e clique em **Criar**.\r\n",
        "2. Na página **Itens da lista**, na seção **Valores normalizados**, digite **luz** e pressione ENTER.\r\n",
        "3. Depois que o valor **luz** for adicionado, na seção **Sinônimos**, digite **lâmpada** e pressione ENTER.\r\n",
        "4. Adicione um segundo item da lista chamado **ventilador** com o sinônimo **ar-condicionado**.\r\n",
        "\r\n",
        "> **Observação**: neste laboratório, use o texto exato com letras maiúsculas ou minúsculas conforme instruído _(exemplo: luz, **não** Luz)_ e não coloque espaços adicionais. \r\n",
        "\r\n",
        "### Criar intenções\r\n",
        "\r\n",
        "Uma *intenção* é uma ação que você quer realizar em uma ou mais entidades, por exemplo, você pode querer acender uma luz ou desligar um ventilador. Neste caso, você definirá duas intenções: uma para ligar um dispositivo e uma para desligar outro. Para cada intenção, você especificará *declarações* que indicam o tipo de linguagem usada para demonstrar a intenção.\r\n",
        "\r\n",
        "> **Observação**: neste laboratório, use o texto exato com letras maiúsculas ou minúsculas conforme instruído _(exemplo: \"acender a luz\", **não** \"Acender a luz\")_ e não coloque espaços adicionais. \r\n",
        "\r\n",
        "1. No painel à esquerda, clique em **Intenções**. Em seguida, clique em **Criar**, adicione uma intenção com o nome **ligar** e clique em **Concluir**.\r\n",
        "2. No título **Exemplos** e subtítulo **Exemplo de entrada de usuário**, digite a declaração ***acender a luz*** e pressione **Enter** para enviá-la para a lista.\r\n",
        "3. Na declaração *acender a luz*, clique na palavra \"luz\" e atribua esse enunciado ao valor **luz** da entidade **dispositivo**.\r\n",
        "\r\n",
        "![Como atribuir a palavra \"luz\" ao valor da entidade.](./images/assign_entity.jpg)\r\n",
        "\r\n",
        "4. Adicione uma segunda declaração à intenção **ligar**, com a frase ***ligar o ventilador***. Em seguida, atribua a palavra \"ventilador\" ao valor **ventilador** da entidade **dispositivo**.\r\n",
        "5. No painel à esquerda, clique em **Intenções** e depois em **Criar** para adicionar uma segunda intenção com o nome **desligar**.\r\n",
        "6. Na página **Declarações** da intenção **desligar**, adicione a declaração ***apagar a luz*** e atribua a palavra \"luz\" ao valor **luz** da entidade **dispositivo**.\r\n",
        "7. Adicione uma segunda declaração à intenção **desligar**, com a frase ***desligar o ventilador***. Em seguida, conecte a palavra \"ventilador\" ao valor **ventilador** da entidade **dispositivo**.\r\n",
        "\r\n",
        "### Treinar e testar o modelo de linguagem\r\n",
        "\r\n",
        "Agora você está pronto para usar os dados inseridos em forma de entidades, intenções e declarações para treinar o modelo de linguagem do seu aplicativo.\r\n",
        "\r\n",
        "1. Na parte superior da página de Reconhecimento vocal do seu aplicativo, clique em **Treinar** para treinar o modelo de linguagem\r\n",
        "2. Quando o modelo estiver treinado, clique em **Testar** e use o painel de teste para visualizar a intenção prevista para as seguintes frases:\r\n",
        "    * *acender a luz*\r\n",
        "    * *desligar o ventilador*\r\n",
        "    * *apagar a lâmpada*\r\n",
        "    * *ligar o ar-condicionado*\r\n",
        "3. Feche o painel Teste.\r\n",
        "    \r\n",
        "### Publicar o modelo e configurar pontos de extremidade\r\n",
        "\r\n",
        "Para usar seu modelo treinado em um aplicativo cliente, é preciso publicá-lo como um ponto de extremidade para o qual os aplicativos clientes poderão enviar novas declarações para que as intenções e entidades sejam previstas.\r\n",
        "\r\n",
        "1. Na parte superior da página de Reconhecimento vocal do seu aplicativo, clique em **Publicar**. Depois, selecione **Slot de produção** e clique em **Concluir**.\r\n",
        "\r\n",
        "2. Depois que o modelo for publicado, na parte superior da página de Reconhecimento vocal do seu aplicativo, clique em **Gerenciar**. Em seguida, na guia **Configurações**, anote a **ID do Aplicativo**. Copie essas informações e cole no código abaixo para substituir **YOUR_LU_APP_ID**.\r\n",
        "\r\n",
        "3. Na guia **Recursos do Azure**, identifique a **Chave primária** e o **URL do ponto de extremidade** do seu recurso de previsão. Copie essas informações e cole no código abaixo, substituindo **YOUR_LU_KEY** e **YOUR_LU_ENDPOINT**.\r\n",
        "\r\n",
        "4. Execute a célula abaixo clicando no botão **Executar célula** (&#9655;) (à esquerda) e, quando solicitado, insira o texto *acender a luz*. O texto é interpretado pelo seu modelo de Reconhecimento vocal e uma imagem apropriada será exibida.\r\n",
        "\r\n",
        "### **(!) Importante**: \r\n",
        "Observe o aviso no topo da janela. Você precisará digitar *acender a luz* e pressionar **enter**. \r\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from python_code import luis\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "try:\n",
        "    # Set up API configuration\n",
        "    luis_app_id = 'YOUR_LU_APP_ID'\n",
        "    luis_key = 'YOUR_LU_KEY'\n",
        "    luis_endpoint = 'YOUR_LU_ENDPOINT'\n",
        "\n",
        "    # prompt for a command\n",
        "    command = input('Please enter a command: \\n')\n",
        "\n",
        "    # get the predicted intent and entity (code in python_code.home_auto.py)\n",
        "    action = luis.get_intent(luis_app_id, luis_key, luis_endpoint, command)\n",
        "\n",
        "    # display an appropriate image\n",
        "    img_name = action + '.jpg'\n",
        "    img = Image.open(os.path.join(\"data\", \"luis\" ,img_name))\n",
        "    plt.axis('off')\n",
        "    plt. imshow(img)\n",
        "except Exception as ex:\n",
        "    print(ex)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [],
        "gather": {
          "logged": 1599696381331
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (!) Verificação \r\n",
        "Você executou a célula acima e digitou a frase *acender a luz* quando solicitado? O aviso será exibido no topo da janela.  \r\n",
        "\r\n",
        "Execute novamente a célula acima, usando as seguintes frases:\r\n",
        "\r\n",
        "* *ligar a luz*\r\n",
        "* *apagar a lâmpada*\r\n",
        "* *ligar o ventilador*\r\n",
        "* *acender a luz*\r\n",
        "* *apagar a luz*\r\n",
        "* *desligar o ventilador*\r\n",
        "* *ligar o ar-condicionado*\r\n",
        "\r\n",
        "Se você executou a célula acima e recebeu uma imagem de ponto de interrogação, talvez tenha usado um texto ou espaçamento um pouco diferente do que foi instruído ao criar uma entidade, intenção ou declaração.\r\n",
        "\r\n",
        "> **Observação**: se estiver curioso a respeito do código usado para recuperar as intenções e entidades do seu aplicativo de Reconhecimento vocal, confira o arquivo **luis.py** na pasta **python_code**."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adicionar Controle de Serviço de Voz\r\n",
        "\r\n",
        "Até agora, vimos como analisar textos. Porém, cada vez mais sistemas de IA permitem que as pessoas se comuniquem com serviços de software por meio do reconhecimento de fala. Para ajudar, o serviço cognitivo de **Fala** oferece uma maneira simples de transformar linguagem falada em texto.\r\n",
        "\r\n",
        "### Criar um recurso dos Serviços Cognitivos\r\n",
        "\r\n",
        "Caso você ainda não tenha um, use as etapas a seguir para criar um recurso dos **Serviços Cognitivos** na sua assinatura do Azure:\r\n",
        "\r\n",
        "> **Observação**: se você já tiver um recurso dos Serviços Cognitivos, abra a página de **Início Rápido** no portal do Azure e copie as respectivas chave e localização para a célula abaixo. Caso contrário, siga as etapas abaixo para criar um.\r\n",
        "\r\n",
        "1. Em outra guia do navegador, abra o portal do Azure em [https://portal.azure.com](https://portal.azure.com), entrando com sua conta Microsoft.\r\n",
        "2. Clique no botão **&#65291;Criar um recurso**, procure *Serviços Cognitivos* e crie um recurso dos **Serviços Cognitivos** com as configurações abaixo:\r\n",
        "    - **Assinatura**: *sua assinatura do Azure*.\r\n",
        "    - **Grupo de recursos**: *Selecione ou crie um grupo de recursos com um nome exclusivo*.\r\n",
        "    - **Região**: *Escolha qualquer região disponível*:\r\n",
        "    - **Nome**: *Insira um nome exclusivo*.\r\n",
        "    - **Tipo de preço**: S0\r\n",
        "    - **Ao marcar essa caixa de seleção, confirmo que o uso desse serviço não será realizado por ou para um departamento de polícia dos Estados Unidos**: Selecionado.\r\n",
        "    - **Confirmo que li e entendi os avisos**: Selecionado.\r\n",
        "3. Aguarde até que a implantação seja concluída. Em seguida, acesse seu recurso de Serviços Cognitivos e, na página de **Início Rápido**, tome nota das chaves e da localização. Você precisará desses dados para conectar o recurso de Serviços Cognitivos em aplicativos clientes.\r\n",
        "\r\n",
        "### Obter a chave e a localização do seu recurso dos Serviços Cognitivos\r\n",
        "\r\n",
        "Para usar seu recurso dos Serviços Cognitivos, os aplicativos clientes precisam da chave de autenticação e da localização:\r\n",
        "\r\n",
        "1. No portal do Azure, na página **Chaves e ponto de extremidade** do seu recurso dos Serviços Cognitivos, copie a **Chave 1** do recurso e cole no código abaixo, substituindo **YOUR_COG_KEY**.\r\n",
        "2. Copie a **Localização** do recurso e cole no código abaixo, substituindo **YOUR_COG_LOCATION**.\r\n",
        ">**Observação**: continue na página **Chave e ponto de extremidade** e copie a **Localização** desta página (por exemplo: _westus_). _Não_ adicione espaços entre as palavras no campo Localização. \r\n",
        "3. Execute o código na célula abaixo. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cog_key = 'YOUR_COG_KEY'\n",
        "cog_location = 'YOUR_COG_LOCATION'\n",
        "\n",
        "print('Ready to use cognitive services in {} using key {}'.format(cog_location, cog_key))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [],
        "gather": {
          "logged": 1599696409914
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora, execute a célula abaixo para transcrever a fala de um arquivo de áudio e usá-la como um comando no seu aplicativo de Reconhecimento vocal."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from python_code import luis\n",
        "from azure.cognitiveservices.speech import SpeechConfig, SpeechRecognizer, AudioConfig\n",
        "from playsound import playsound\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "try:   \n",
        "\n",
        "    # Get spoken command from audio file\n",
        "    file_name = 'light-on.wav'\n",
        "    audio_file = os.path.join('data', 'luis', file_name)\n",
        "\n",
        "    # Configure speech recognizer\n",
        "    speech_config = SpeechConfig(cog_key, cog_location)\n",
        "    audio_config = AudioConfig(filename=audio_file) # Use file instead of default (microphone)\n",
        "    speech_recognizer = SpeechRecognizer(speech_config, audio_config)\n",
        "\n",
        "    # Use a one-time, synchronous call to transcribe the speech\n",
        "    speech = speech_recognizer.recognize_once()\n",
        "\n",
        "    # Get the predicted intent and entity (code in python_code.home_auto.py)\n",
        "    action = luis.get_intent(luis_app_id, luis_key, luis_endpoint, speech.text)\n",
        "\n",
        "    # Get the appropriate image\n",
        "    img_name = action + '.jpg'\n",
        "\n",
        "    # Display image \n",
        "    img = Image.open(os.path.join(\"data\", \"luis\" ,img_name))\n",
        "    plt.axis('off')\n",
        "    plt. imshow(img)\n",
        "    playsound(audio_file)\n",
        "\n",
        "except Exception as ex:\n",
        "    print(ex)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [],
        "gather": {
          "logged": 1599696420498
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experimente modificar a célula acima para usar o arquivo de áudio **light-off.wav**.\r\n",
        "\r\n",
        "## Saiba mais\r\n",
        "\r\n",
        "Saiba mais sobre o Reconhecimento vocal na [documentação do serviço](https://docs.microsoft.com/azure/cognitive-services/luis/)"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}